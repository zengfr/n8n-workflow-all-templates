{
  "meta": {
    "site": "https://github.com/zengfr/n8n-workflow-all-templates",
    "name": "Monitor AI Chat Interactions with Gemini 2.5 and Langfuse Tracing",
    "wechat": "youandme10086",
    "id": 4972,
    "update_time": "2025-11-10"
  },
  "nodes": [
    {
      "id": "c26d363f-53d2-446b-98db-d68a4b947bc5",
      "name": "When chat message received",
      "type": "@n8n/n8n-nodes-langchain.chatTrigger",
      "position": [
        880,
        540
      ],
      "webhookId": "3917af54-131f-41c5-a250-b32e0ff9dc5f",
      "parameters": {
        "options": {}
      },
      "typeVersion": 1.1
    },
    {
      "id": "01f4841f-2e4d-4beb-8f19-258cb4e8f988",
      "name": "gemini-2.5",
      "type": "@n8n/n8n-nodes-langchain.lmChatGoogleGemini",
      "position": [
        1188,
        960
      ],
      "parameters": {
        "options": {
          "temperature": 0.4
        },
        "modelName": "models/gemini-2.5-flash-preview-05-20"
      },
      "credentials": {
        "googlePalmApi": {
          "id": "XmPj4vZ604DaosrU",
          "name": "gemini-personal"
        }
      },
      "typeVersion": 1
    },
    {
      "id": "f63d8101-0b24-4917-9126-ceccc926cb3c",
      "name": "mem",
      "type": "@n8n/n8n-nodes-langchain.memoryBufferWindow",
      "position": [
        1396,
        760
      ],
      "parameters": {
        "contextWindowLength": 100
      },
      "typeVersion": 1.3
    },
    {
      "id": "c5c637fd-962d-4c2c-bb7a-c35faae85ee1",
      "name": "Langfuse LLM",
      "type": "@n8n/n8n-nodes-langchain.code",
      "position": [
        1100,
        762.5
      ],
      "parameters": {
        "code": {
          "supplyData": {
            "code": "const { CallbackHandler } = require(\"langfuse-langchain\");\nconst { initChatModel } = require(\"langchain/chat_models/universal\");\n\n// Get connected model\nconst model = await this.getInputConnectionData(\"ai_languageModel\", 0);\nconst modelProvider = model.lc_namespace[2].replace(\"_\", \"-\");\nconst modelName = model.model;\nconst temperature = model.temperature;\n\n// Initialize Langfuse callback handler\nconst sessionId = $input.item.json.sessionId;\nconst langfuseHandler = new CallbackHandler({\n  sessionId,\n});\n\nconst llm = await initChatModel(modelName, {\n  temperature,\n  modelProvider,\n  callbacks: [langfuseHandler],\n});\n\nreturn llm;\n"
          }
        },
        "inputs": {
          "input": [
            {
              "type": "ai_languageModel",
              "required": true,
              "maxConnections": 1
            }
          ]
        },
        "outputs": {
          "output": [
            {
              "type": "ai_languageModel"
            }
          ]
        }
      },
      "typeVersion": 1
    },
    {
      "id": "c5f53fbe-5603-4384-b4fa-076a69a1f6aa",
      "name": "AI Agent",
      "type": "@n8n/n8n-nodes-langchain.agent",
      "position": [
        1204,
        540
      ],
      "parameters": {
        "options": {}
      },
      "typeVersion": 2
    }
  ],
  "pinData": {},
  "connections": {
    "mem": {
      "ai_memory": [
        [
          {
            "node": "AI Agent",
            "type": "ai_memory",
            "index": 0
          }
        ]
      ]
    },
    "AI Agent": {
      "main": [
        []
      ]
    },
    "gemini-2.5": {
      "ai_languageModel": [
        [
          {
            "node": "Langfuse LLM",
            "type": "ai_languageModel",
            "index": 0
          }
        ]
      ]
    },
    "Langfuse LLM": {
      "ai_languageModel": [
        [
          {
            "node": "AI Agent",
            "type": "ai_languageModel",
            "index": 0
          }
        ]
      ]
    },
    "When chat message received": {
      "main": [
        [
          {
            "node": "AI Agent",
            "type": "main",
            "index": 0
          }
        ]
      ]
    }
  }
}