{
  "meta": {
    "site": "https://github.com/zengfr/n8n-workflow-all-templates",
    "name": "Process Multiple Prompts in Parallel with Azure OpenAI Batch API",
    "wechat": "youandme10086",
    "id": 3537,
    "update_time": "2025-11-10"
  },
  "nodes": [
    {
      "id": "869e151b-e97f-41a5-bd70-680d8f7cb3f9",
      "name": "When Executed by Another Workflow",
      "type": "n8n-nodes-base.executeWorkflowTrigger",
      "position": [
        -2120,
        215
      ],
      "parameters": {},
      "typeVersion": 1.1
    },
    {
      "id": "2a1b507a-bce8-4d9e-9f9d-49e0a05d62cc",
      "name": "Parse response",
      "type": "n8n-nodes-base.code",
      "notes": "JSONL separated by newlines",
      "position": [
        520,
        -160
      ],
      "parameters": {},
      "notesInFlow": true,
      "typeVersion": 2
    },
    {
      "id": "ed90fe23-b862-4279-93b5-cc15b54c7858",
      "name": "If ended processing",
      "type": "n8n-nodes-base.if",
      "position": [
        80,
        -60
      ],
      "parameters": {},
      "typeVersion": 2.2
    },
    {
      "id": "423ab642-2a51-41f7-ba4a-e2e906827e28",
      "name": "Sticky Note",
      "type": "n8n-nodes-base.stickyNote",
      "position": [
        -3240,
        -120
      ],
      "parameters": {
        "content": ""
      },
      "typeVersion": 1
    },
    {
      "id": "23ad6403-83a9-4517-b33e-8ff81a20d5f0",
      "name": "Sticky Note1",
      "type": "n8n-nodes-base.stickyNote",
      "position": [
        920,
        -380
      ],
      "parameters": {
        "content": ""
      },
      "typeVersion": 1
    },
    {
      "id": "968727f2-8eb1-49a6-b270-8c9e5953c666",
      "name": "Batch Status Poll Interval",
      "type": "n8n-nodes-base.wait",
      "position": [
        300,
        40
      ],
      "webhookId": "8b5afff6-b9eb-414d-8faf-1d0626509021",
      "parameters": {},
      "typeVersion": 1.1
    },
    {
      "id": "e3e64104-d457-4099-95a9-a99910c37f2d",
      "name": "Sticky Note2",
      "type": "n8n-nodes-base.stickyNote",
      "position": [
        -2200,
        700
      ],
      "parameters": {
        "content": ""
      },
      "typeVersion": 1
    },
    {
      "id": "9427eb5f-46db-47f2-b0a8-d18fe75b7010",
      "name": "Run example",
      "type": "n8n-nodes-base.manualTrigger",
      "position": [
        -2120,
        800
      ],
      "parameters": {},
      "typeVersion": 1
    },
    {
      "id": "fdedc622-3a0d-4bc8-ab4b-a8a157110e9c",
      "name": "One query example",
      "type": "n8n-nodes-base.set",
      "position": [
        -1446,
        950
      ],
      "parameters": {},
      "typeVersion": 3.4
    },
    {
      "id": "058a4f04-4f60-4e31-9ad4-edc83829b2b1",
      "name": "Delete original properties",
      "type": "n8n-nodes-base.set",
      "position": [
        -552,
        950
      ],
      "parameters": {},
      "typeVersion": 3.4
    },
    {
      "id": "cd1dfc83-968f-4661-8ca5-c46b67b3ea52",
      "name": "Construct 'requests' array",
      "type": "n8n-nodes-base.aggregate",
      "position": [
        -112,
        800
      ],
      "parameters": {},
      "typeVersion": 1
    },
    {
      "id": "cba0fe20-e318-418a-87a1-c3515662bf34",
      "name": "Build batch 'request' object for single query",
      "type": "n8n-nodes-base.code",
      "position": [
        -772,
        950
      ],
      "parameters": {},
      "typeVersion": 2
    },
    {
      "id": "f556be36-f8be-4a71-bd0a-b3b54084a120",
      "name": "Simple Memory Store",
      "type": "@n8n/n8n-nodes-langchain.memoryBufferWindow",
      "position": [
        -1436,
        770
      ],
      "parameters": {},
      "typeVersion": 1.3
    },
    {
      "id": "81f33e27-8b8a-461b-8592-43de9ac3d3e8",
      "name": "Fill Chat Memory with example data",
      "type": "@n8n/n8n-nodes-langchain.memoryManager",
      "position": [
        -1524,
        550
      ],
      "parameters": {},
      "typeVersion": 1.1
    },
    {
      "id": "c22c3e0f-4242-42ed-b723-84fb03b0d418",
      "name": "Build batch 'request' object from Chat Memory and execution data",
      "type": "n8n-nodes-base.code",
      "position": [
        -552,
        650
      ],
      "parameters": {},
      "typeVersion": 2
    },
    {
      "id": "660d0dc8-6976-41b5-835c-e4d0a60c5aa3",
      "name": "Load Chat Memory Data",
      "type": "@n8n/n8n-nodes-langchain.memoryManager",
      "position": [
        -1148,
        650
      ],
      "parameters": {},
      "typeVersion": 1.1
    },
    {
      "id": "e3e79acb-b0a7-494c-843c-6934e15c6775",
      "name": "First Prompt Result",
      "type": "n8n-nodes-base.executionData",
      "position": [
        768,
        700
      ],
      "parameters": {},
      "typeVersion": 1
    },
    {
      "id": "74e7ea77-0cbd-411d-8191-4ce5a45e0854",
      "name": "Second Prompt Result",
      "type": "n8n-nodes-base.executionData",
      "position": [
        768,
        900
      ],
      "parameters": {},
      "typeVersion": 1
    },
    {
      "id": "617f424c-b466-4ea5-a073-20fb95b0f81f",
      "name": "Split Out Parsed Results",
      "type": "n8n-nodes-base.splitOut",
      "position": [
        740,
        -160
      ],
      "parameters": {},
      "typeVersion": 1
    },
    {
      "id": "29f1c4ae-4211-438b-b8d0-8a2c5e634f5e",
      "name": "Filter Second Prompt Results",
      "type": "n8n-nodes-base.filter",
      "position": [
        548,
        900
      ],
      "parameters": {},
      "typeVersion": 2.2
    },
    {
      "id": "77485de9-feb1-4f5c-be27-e66ad00d70b4",
      "name": "Filter First Prompt Results",
      "type": "n8n-nodes-base.filter",
      "position": [
        548,
        700
      ],
      "parameters": {},
      "typeVersion": 2.2
    },
    {
      "id": "ab0b40dd-7af4-4239-a033-77747b544fcc",
      "name": "Sticky Note6",
      "type": "n8n-nodes-base.stickyNote",
      "position": [
        -1240,
        520
      ],
      "parameters": {
        "content": ""
      },
      "typeVersion": 1
    },
    {
      "id": "d1ec862e-a33f-44c4-a259-5102e4b37aac",
      "name": "Sticky Note7",
      "type": "n8n-nodes-base.stickyNote",
      "position": [
        -1539,
        890
      ],
      "parameters": {
        "content": ""
      },
      "typeVersion": 1
    },
    {
      "id": "e549a257-6667-417c-9978-c31889b9b3e7",
      "name": "Sticky Note8",
      "type": "n8n-nodes-base.stickyNote",
      "position": [
        -1900,
        520
      ],
      "parameters": {
        "content": ""
      },
      "typeVersion": 1
    },
    {
      "id": "43b930e8-ec5c-4a43-9eaf-3ff98d2caf49",
      "name": "Sticky Note10",
      "type": "n8n-nodes-base.stickyNote",
      "position": [
        480,
        560
      ],
      "parameters": {
        "content": ""
      },
      "typeVersion": 1
    },
    {
      "id": "e3a48cbd-ba6a-4d04-8bc7-056d141472b6",
      "name": "Sticky Note11",
      "type": "n8n-nodes-base.stickyNote",
      "position": [
        480,
        860
      ],
      "parameters": {
        "content": ""
      },
      "typeVersion": 1
    },
    {
      "id": "76a3f367-20d1-417e-85c3-65c1170199fd",
      "name": "Join two example requests into array",
      "type": "n8n-nodes-base.merge",
      "position": [
        -332,
        800
      ],
      "parameters": {},
      "typeVersion": 3.1
    },
    {
      "id": "85e66754-6a76-46d6-bc48-dff66edc3c6b",
      "name": "Truncate Chat Memory",
      "type": "@n8n/n8n-nodes-langchain.memoryManager",
      "notes": "ensure clean state",
      "position": [
        -1900,
        650
      ],
      "parameters": {},
      "notesInFlow": true,
      "typeVersion": 1.1
    },
    {
      "id": "544e9cbe-8bd3-4bb2-98a8-ee808491f21d",
      "name": "JSON requests to JSONL",
      "type": "n8n-nodes-base.code",
      "position": [
        -1460,
        215
      ],
      "parameters": {},
      "typeVersion": 2
    },
    {
      "id": "01d04a44-4cc6-4290-88ba-402ef0ab5a37",
      "name": "Change file name and mimetype",
      "type": "n8n-nodes-base.code",
      "position": [
        -1020,
        15
      ],
      "parameters": {},
      "typeVersion": 2
    },
    {
      "id": "95901367-2db2-47c3-8ba0-a1afd8a2d853",
      "name": "If upload processed",
      "type": "n8n-nodes-base.if",
      "position": [
        -360,
        215
      ],
      "parameters": {},
      "typeVersion": 2.2
    },
    {
      "id": "b8ee0ab8-d4b5-411e-9a2e-8bac6b99091e",
      "name": "File Upload Poll Interval",
      "type": "n8n-nodes-base.wait",
      "position": [
        -140,
        140
      ],
      "webhookId": "240df7df-4bfc-46e8-9f49-6d10236516ae",
      "parameters": {},
      "typeVersion": 1.1
    },
    {
      "id": "ed8713c6-d4db-4c1f-86d4-e3bcfa0a1a3d",
      "name": "Track file upload status",
      "type": "n8n-nodes-base.httpRequest",
      "notes": "GET /openai/files/file_id",
      "position": [
        80,
        290
      ],
      "parameters": {},
      "notesInFlow": true,
      "typeVersion": 4.2
    },
    {
      "id": "f9701dd8-3f8e-4a3c-9e92-9055ae7ddebc",
      "name": "Create batch job",
      "type": "n8n-nodes-base.httpRequest",
      "notes": "POST /openai/batches",
      "position": [
        -140,
        -60
      ],
      "parameters": {},
      "notesInFlow": true,
      "typeVersion": 4.2
    },
    {
      "id": "6000210c-753b-46b7-b0db-d293907cc999",
      "name": "Track batch job progress",
      "type": "n8n-nodes-base.httpRequest",
      "notes": "GET /openai/batches/batch_id",
      "position": [
        520,
        115
      ],
      "parameters": {},
      "notesInFlow": true,
      "typeVersion": 4.2
    },
    {
      "id": "c477b4fd-252a-4637-811f-f5e996a8cff9",
      "name": "Retrieve batch job output file",
      "type": "n8n-nodes-base.httpRequest",
      "notes": "GET /openai/files/output_file_id/content",
      "position": [
        300,
        -160
      ],
      "parameters": {},
      "notesInFlow": true,
      "typeVersion": 4.2
    },
    {
      "id": "b451e537-076f-4344-b6e0-d8fd17cb7a92",
      "name": "Set desired 'api-version'",
      "type": "n8n-nodes-base.set",
      "notes": "2025-03-01-preview",
      "position": [
        108,
        800
      ],
      "parameters": {},
      "notesInFlow": true,
      "typeVersion": 3.4
    },
    {
      "id": "000a3227-2ab8-4896-9d88-a416ee9a8e87",
      "name": "Merge File and API Version",
      "type": "n8n-nodes-base.merge",
      "position": [
        -800,
        220
      ],
      "parameters": {},
      "typeVersion": 3.1
    },
    {
      "id": "4944a924-b680-4216-bdd5-7ffa50627580",
      "name": "Remove original requests",
      "type": "n8n-nodes-base.set",
      "notes": "Keep 'api-version'",
      "position": [
        -1020,
        215
      ],
      "parameters": {},
      "notesInFlow": true,
      "typeVersion": 3.4
    },
    {
      "id": "0c240fcd-ce2b-4daa-84eb-7624de36bb79",
      "name": "Convert requests jsonl to File",
      "type": "n8n-nodes-base.convertToFile",
      "notes": "%current-datetime%.jsonl",
      "position": [
        -1240,
        15
      ],
      "parameters": {},
      "notesInFlow": true,
      "typeVersion": 1.1
    },
    {
      "id": "31735e90-3d18-4228-8962-278ff3d75791",
      "name": "Append custom_id for single query example",
      "type": "n8n-nodes-base.set",
      "notes": "custom_id",
      "position": [
        -1070,
        950
      ],
      "parameters": {},
      "notesInFlow": true,
      "typeVersion": 3.4
    },
    {
      "id": "c096effd-2b83-4edc-bbb3-fa87306f7477",
      "name": "Append custom_id for chat memory example",
      "type": "n8n-nodes-base.set",
      "notes": "custom_id",
      "position": [
        -772,
        650
      ],
      "parameters": {},
      "notesInFlow": true,
      "typeVersion": 3.4
    },
    {
      "id": "f7703a6d-d92b-4f3a-9780-bd9b933f277c",
      "name": "Setup defaults",
      "type": "n8n-nodes-base.set",
      "position": [
        -1900,
        215
      ],
      "parameters": {},
      "typeVersion": 3.4
    },
    {
      "id": "77166b70-1522-48b7-b1e2-09440d21a17f",
      "name": "Set defaults if not set already",
      "type": "n8n-nodes-base.code",
      "position": [
        -1680,
        215
      ],
      "parameters": {},
      "typeVersion": 2
    },
    {
      "id": "28a87bf4-db58-4d92-b73b-a79104611594",
      "name": "Execute Workflow 'Process Multiple Prompts in Parallel with Azure OpenAI Batch API'",
      "type": "n8n-nodes-base.executeWorkflow",
      "notes": "See above",
      "position": [
        328,
        800
      ],
      "parameters": {},
      "notesInFlow": true,
      "typeVersion": 1.2
    },
    {
      "id": "155554ae-c9c6-4992-beb9-3581c5f630bc",
      "name": "Sticky Note3",
      "type": "n8n-nodes-base.stickyNote",
      "position": [
        -3240,
        -540
      ],
      "parameters": {
        "content": ""
      },
      "typeVersion": 1
    },
    {
      "id": "f3c6575a-8919-4617-bc13-6fc77f79e2be",
      "name": "Sticky Note4",
      "type": "n8n-nodes-base.stickyNote",
      "position": [
        40,
        620
      ],
      "parameters": {
        "content": ""
      },
      "typeVersion": 1
    },
    {
      "id": "bc289c2b-0a49-4742-b021-7079adbdb2e1",
      "name": "Upload batch file",
      "type": "n8n-nodes-base.httpRequest",
      "notes": "POST /openai/files",
      "position": [
        -580,
        215
      ],
      "parameters": {},
      "notesInFlow": true,
      "typeVersion": 4.2
    },
    {
      "id": "8a017384-4e27-4140-8592-138bda05f77f",
      "name": "Sticky Note5",
      "type": "n8n-nodes-base.stickyNote",
      "position": [
        -1960,
        -540
      ],
      "parameters": {
        "content": ""
      },
      "typeVersion": 1
    },
    {
      "id": "df646ebb-d595-441b-899f-2b894e6c92e4",
      "name": "Sticky Note9",
      "type": "n8n-nodes-base.stickyNote",
      "position": [
        -600,
        120
      ],
      "parameters": {
        "content": ""
      },
      "typeVersion": 1
    },
    {
      "id": "d78b9a51-2fc0-4856-b4e9-a66ce1019a44",
      "name": "Sticky Note12",
      "type": "n8n-nodes-base.stickyNote",
      "position": [
        -200,
        -140
      ],
      "parameters": {
        "content": ""
      },
      "typeVersion": 1
    },
    {
      "id": "e6e8d6ad-3292-4424-9ba9-4b7e24cfda99",
      "name": "Sticky Note13",
      "type": "n8n-nodes-base.stickyNote",
      "position": [
        440,
        20
      ],
      "parameters": {
        "content": ""
      },
      "typeVersion": 1
    },
    {
      "id": "50221e19-f010-4f9e-89b2-4dfe1db4d057",
      "name": "Sticky Note14",
      "type": "n8n-nodes-base.stickyNote",
      "position": [
        260,
        -280
      ],
      "parameters": {
        "content": ""
      },
      "typeVersion": 1
    }
  ],
  "pinData": {
    "Run example": [
      {}
    ],
    "Filter First Prompt Results": [
      {
        "error": null,
        "response": {
          "body": {
            "id": "chatcmpl-BLniBny5pbUeHiFzUSBALN5xzyv1z",
            "model": "gpt-4o-mini-2024-07-18",
            "usage": {
              "total_tokens": 103,
              "prompt_tokens": 51,
              "completion_tokens": 52,
              "prompt_tokens_details": {
                "audio_tokens": 0,
                "cached_tokens": 0
              },
              "completion_tokens_details": {
                "audio_tokens": 0,
                "reasoning_tokens": 0,
                "accepted_prediction_tokens": 0,
                "rejected_prediction_tokens": 0
              }
            },
            "object": "chat.completion",
            "choices": [
              {
                "index": 0,
                "message": {
                  "role": "assistant",
                  "content": "Sure! Did you know that the best-selling video game of all time is \"Minecraft\"? As of 2023, it has sold over 300 million copies across various platforms since its release in 2011, showcasing its immense popularity and enduring appeal!",
                  "refusal": null
                },
                "logprobs": null,
                "finish_reason": "stop",
                "content_filter_results": {
                  "hate": {
                    "filtered": false,
                    "severity": "safe"
                  },
                  "sexual": {
                    "filtered": false,
                    "severity": "safe"
                  },
                  "violence": {
                    "filtered": false,
                    "severity": "safe"
                  },
                  "self_harm": {
                    "filtered": false,
                    "severity": "safe"
                  }
                }
              }
            ],
            "created": 1744535679,
            "system_fingerprint": "fp_ded0d14823",
            "prompt_filter_results": [
              {
                "prompt_index": 0,
                "content_filter_results": {
                  "hate": {
                    "filtered": false,
                    "severity": "safe"
                  },
                  "sexual": {
                    "filtered": false,
                    "severity": "safe"
                  },
                  "violence": {
                    "filtered": false,
                    "severity": "safe"
                  },
                  "jailbreak": {
                    "detected": false,
                    "filtered": false
                  },
                  "self_harm": {
                    "filtered": false,
                    "severity": "safe"
                  }
                }
              }
            ]
          },
          "request_id": "7c0c66c4-7db2-4de7-9f13-b548eb93bd7a",
          "status_code": 200
        },
        "custom_id": "first-prompt-in-my-batch"
      }
    ],
    "When Executed by Another Workflow": [
      {
        "requests": [
          {
            "params": {
              "messages": [
                {
                  "role": "system",
                  "content": "You are a helpful AI assistant"
                },
                {
                  "role": "user",
                  "content": "Hey Claude, tell me a short fun fact about video games!"
                },
                {
                  "role": "assistant",
                  "content": "short fun fact about video games!"
                },
                {
                  "role": "user",
                  "content": "No, an actual fun fact"
                }
              ]
            },
            "custom_id": "first-prompt-in-my-batch"
          },
          {
            "params": {
              "messages": [
                {
                  "role": "user",
                  "content": "Hey Claude, tell me a short fun fact about bees!"
                }
              ]
            },
            "custom_id": "second-prompt-in-my-batch"
          }
        ],
        "api-version": "2025-03-01-preview"
      }
    ]
  },
  "connections": {
    "Run example": {
      "main": [
        [
          {
            "node": "One query example",
            "type": "main",
            "index": 0
          },
          {
            "node": "Truncate Chat Memory",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Parse response": {
      "main": [
        [
          {
            "node": "Split Out Parsed Results",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Setup defaults": {
      "main": [
        [
          {
            "node": "Set defaults if not set already",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Create batch job": {
      "main": [
        [
          {
            "node": "If ended processing",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "One query example": {
      "main": [
        [
          {
            "node": "Append custom_id for single query example",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Upload batch file": {
      "main": [
        [
          {
            "node": "If upload processed",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "If ended processing": {
      "main": [
        [
          {
            "node": "Retrieve batch job output file",
            "type": "main",
            "index": 0
          }
        ],
        [
          {
            "node": "Batch Status Poll Interval",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "If upload processed": {
      "main": [
        [
          {
            "node": "Create batch job",
            "type": "main",
            "index": 0
          }
        ],
        [
          {
            "node": "File Upload Poll Interval",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Simple Memory Store": {
      "ai_memory": [
        [
          {
            "node": "Load Chat Memory Data",
            "type": "ai_memory",
            "index": 0
          },
          {
            "node": "Fill Chat Memory with example data",
            "type": "ai_memory",
            "index": 0
          },
          {
            "node": "Truncate Chat Memory",
            "type": "ai_memory",
            "index": 0
          }
        ]
      ]
    },
    "Truncate Chat Memory": {
      "main": [
        [
          {
            "node": "Fill Chat Memory with example data",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Load Chat Memory Data": {
      "main": [
        [
          {
            "node": "Append custom_id for chat memory example",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "JSON requests to JSONL": {
      "main": [
        [
          {
            "node": "Remove original requests",
            "type": "main",
            "index": 0
          },
          {
            "node": "Convert requests jsonl to File",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Remove original requests": {
      "main": [
        [
          {
            "node": "Merge File and API Version",
            "type": "main",
            "index": 1
          }
        ]
      ]
    },
    "Track batch job progress": {
      "main": [
        [
          {
            "node": "If ended processing",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Track file upload status": {
      "main": [
        [
          {
            "node": "If upload processed",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "File Upload Poll Interval": {
      "main": [
        [
          {
            "node": "Track file upload status",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Set desired 'api-version'": {
      "main": [
        [
          {
            "node": "Execute Workflow 'Process Multiple Prompts in Parallel with Azure OpenAI Batch API'",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Batch Status Poll Interval": {
      "main": [
        [
          {
            "node": "Track batch job progress",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Construct 'requests' array": {
      "main": [
        [
          {
            "node": "Set desired 'api-version'",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Delete original properties": {
      "main": [
        [
          {
            "node": "Join two example requests into array",
            "type": "main",
            "index": 1
          }
        ]
      ]
    },
    "Merge File and API Version": {
      "main": [
        [
          {
            "node": "Upload batch file",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Filter First Prompt Results": {
      "main": [
        [
          {
            "node": "First Prompt Result",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Filter Second Prompt Results": {
      "main": [
        [
          {
            "node": "Second Prompt Result",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Change file name and mimetype": {
      "main": [
        [
          {
            "node": "Merge File and API Version",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Convert requests jsonl to File": {
      "main": [
        [
          {
            "node": "Change file name and mimetype",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Retrieve batch job output file": {
      "main": [
        [
          {
            "node": "Parse response",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Set defaults if not set already": {
      "main": [
        [
          {
            "node": "JSON requests to JSONL",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "When Executed by Another Workflow": {
      "main": [
        [
          {
            "node": "Setup defaults",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Fill Chat Memory with example data": {
      "main": [
        [
          {
            "node": "Load Chat Memory Data",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Join two example requests into array": {
      "main": [
        [
          {
            "node": "Construct 'requests' array",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Append custom_id for chat memory example": {
      "main": [
        [
          {
            "node": "Build batch 'request' object from Chat Memory and execution data",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Append custom_id for single query example": {
      "main": [
        [
          {
            "node": "Build batch 'request' object for single query",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Build batch 'request' object for single query": {
      "main": [
        [
          {
            "node": "Delete original properties",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Build batch 'request' object from Chat Memory and execution data": {
      "main": [
        [
          {
            "node": "Join two example requests into array",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Execute Workflow 'Process Multiple Prompts in Parallel with Azure OpenAI Batch API'": {
      "main": [
        [
          {
            "node": "Filter First Prompt Results",
            "type": "main",
            "index": 0
          },
          {
            "node": "Filter Second Prompt Results",
            "type": "main",
            "index": 0
          }
        ]
      ]
    }
  }
}