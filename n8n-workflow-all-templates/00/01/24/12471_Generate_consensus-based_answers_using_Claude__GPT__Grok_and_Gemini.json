{
  "id": "FeqLfdT3r8NYWKlC",
  "meta": {
    "site": "https://github.com/zengfr/n8n-workflow-all-templates",
    "name": "Generate consensus-based answers using Claude, GPT, Grok and Gemini",
    "wechat": "youandme10086",
    "id": 12471,
    "update_time": "2026-02-13"
  },
  "name": "LLM Council",
  "tags": [
    {
      "id": "k0gyhK57BQktK4yA",
      "name": "Technical",
      "createdAt": "2025-09-04T10:44:14.340Z",
      "updatedAt": "2025-09-04T11:13:29.041Z"
    }
  ],
  "nodes": [
    {
      "id": "8cf3f133-6f14-43e6-91f1-5949d44b230f",
      "name": "When chat message received",
      "type": "@n8n/n8n-nodes-langchain.chatTrigger",
      "position": [
        -1552,
        64
      ],
      "webhookId": "d5cb9a71-2e2f-4000-bc38-c115b54721e8",
      "parameters": {
        "options": {}
      },
      "typeVersion": 1.4
    },
    {
      "id": "b142fc0d-12d3-4f3a-8114-fe56c09c894f",
      "name": "Basic LLM Chain",
      "type": "@n8n/n8n-nodes-langchain.chainLlm",
      "position": [
        -1216,
        -448
      ],
      "parameters": {
        "batching": {},
        "messages": {
          "messageValues": [
            {
              "message": "Give an answer in English"
            }
          ]
        }
      },
      "typeVersion": 1.7
    },
    {
      "id": "b95480e3-4fcd-4bfe-873b-dd6718fc2aeb",
      "name": "Basic LLM Chain1",
      "type": "@n8n/n8n-nodes-langchain.chainLlm",
      "position": [
        -1216,
        -96
      ],
      "parameters": {
        "batching": {},
        "messages": {
          "messageValues": [
            {
              "message": "Give an answer in English"
            }
          ]
        }
      },
      "typeVersion": 1.7
    },
    {
      "id": "f600cb1b-c72f-4fe1-a477-fe0c84a7dc63",
      "name": "Basic LLM Chain2",
      "type": "@n8n/n8n-nodes-langchain.chainLlm",
      "position": [
        -1216,
        240
      ],
      "parameters": {
        "batching": {},
        "messages": {
          "messageValues": [
            {
              "message": "Give an answer in English"
            }
          ]
        }
      },
      "typeVersion": 1.7
    },
    {
      "id": "c2a852dc-9201-4224-baf5-4a1862b5b6fb",
      "name": "Basic LLM Chain3",
      "type": "@n8n/n8n-nodes-langchain.chainLlm",
      "position": [
        -1216,
        576
      ],
      "parameters": {
        "batching": {},
        "messages": {
          "messageValues": [
            {
              "message": "Give an answer in English"
            }
          ]
        }
      },
      "typeVersion": 1.7
    },
    {
      "id": "070699ee-c2db-4676-bd74-3447a9f66f5c",
      "name": "Merge",
      "type": "n8n-nodes-base.merge",
      "position": [
        -208,
        512
      ],
      "parameters": {
        "mode": "combine",
        "options": {},
        "combineBy": "combineByPosition",
        "numberInputs": 4
      },
      "typeVersion": 3.2
    },
    {
      "id": "03c5d08a-eb6c-46d8-97a7-abd34ec3f92e",
      "name": "Merge1",
      "type": "n8n-nodes-base.merge",
      "position": [
        -208,
        192
      ],
      "parameters": {
        "mode": "combine",
        "options": {},
        "combineBy": "combineByPosition",
        "numberInputs": 4
      },
      "typeVersion": 3.2
    },
    {
      "id": "e224a52d-0041-4abe-94f7-3f827832ca67",
      "name": "Merge3",
      "type": "n8n-nodes-base.merge",
      "position": [
        -208,
        -432
      ],
      "parameters": {
        "mode": "combine",
        "options": {},
        "combineBy": "combineByPosition",
        "numberInputs": 4
      },
      "typeVersion": 3.2
    },
    {
      "id": "dea776a4-ecc1-4a5d-a72a-3b7cde77893d",
      "name": "gemini1",
      "type": "@n8n/n8n-nodes-langchain.lmChatOpenRouter",
      "position": [
        -1216,
        736
      ],
      "parameters": {
        "model": "google/gemini-3-flash-preview",
        "options": {
          "maxTokens": 4000,
          "temperature": 0.7
        }
      },
      "credentials": {
        "openRouterApi": {
          "id": "1c8GWQkBiu8PxBdr",
          "name": "OpenRouter account"
        }
      },
      "typeVersion": 1
    },
    {
      "id": "635fe200-5ca0-44a5-931a-f688bcbdc662",
      "name": "Merge4",
      "type": "n8n-nodes-base.merge",
      "position": [
        608,
        32
      ],
      "parameters": {
        "mode": "combine",
        "options": {},
        "combineBy": "combineByPosition",
        "numberInputs": 4
      },
      "typeVersion": 3.2
    },
    {
      "id": "674fd520-4610-439a-982e-99d1b1d58bb6",
      "name": "Basic LLM Chain8",
      "type": "@n8n/n8n-nodes-langchain.chainLlm",
      "position": [
        1088,
        64
      ],
      "parameters": {
        "text": "=You are the Chairman of an LLM Council. Multiple AI models have provided responses to a user's question, and then ranked each other's responses.\n\nOriginal Question: {{ $('When chat message received').item.json.chatInput }}\n\nSTAGE 1 - Individual Responses:\nModel: anthropic/claud\nResponse: {{ $('response a').item.json.text_A }}\n\nModel: openai/gpt\nResponse: {{ $('response b').item.json.text_B }}\n\nModel: x-ai/grok\nResponse: {{ $('response c').item.json.text_C }}\n\nModel: google/gemini\nResponse: {{ $('response d').item.json.text_D }}\n\nSTAGE 2 - Peer Rankings:\nModel: anthropic/claud\nRanking: {{ $('evaluate_a').item.json.Raiting_1 }}\n\nModel: openai/gpt\nRanking: {{ $('evaluate_b').item.json.Raiting_2 }}\n\nModel: x-ai/grok\nRanking: {{ $('evaluate_c').item.json.Raiting_3 }}\n\nModel: google/gemini\nRanking: {{ $('evaluate_d').item.json.Raiting_4 }}\n\nYour task as Chairman is to synthesize all of this information into a single, comprehensive, accurate answer to the user's original question. \n\nConsider:\n- The individual responses and their insights\n- The peer rankings and what they reveal about response quality\n- Any patterns of agreement or disagreement\n\nProvide a clear, well-reasoned final answer that represents the council's collective wisdom:",
        "batching": {},
        "promptType": "define"
      },
      "typeVersion": 1.7
    },
    {
      "id": "c1355eee-64bc-4277-8e76-5b38ce176b9e",
      "name": "response a",
      "type": "n8n-nodes-base.set",
      "position": [
        -896,
        -448
      ],
      "parameters": {
        "options": {},
        "assignments": {
          "assignments": [
            {
              "id": "d1ddeb46-5ee6-4bd5-9a38-d7fd7b1b193e",
              "name": "text_A",
              "type": "string",
              "value": "={{ $json.text }}"
            }
          ]
        }
      },
      "typeVersion": 3.4
    },
    {
      "id": "69ea4488-e720-4933-9e12-6870c5ba0b06",
      "name": "response b",
      "type": "n8n-nodes-base.set",
      "position": [
        -896,
        -96
      ],
      "parameters": {
        "options": {},
        "assignments": {
          "assignments": [
            {
              "id": "c7b84760-65b0-40bf-b516-055096148e80",
              "name": "text_B",
              "type": "string",
              "value": "={{ $json.text }}"
            }
          ]
        }
      },
      "typeVersion": 3.4
    },
    {
      "id": "7db542b5-781c-4f39-936d-ed1dfad6c042",
      "name": "response c",
      "type": "n8n-nodes-base.set",
      "position": [
        -896,
        240
      ],
      "parameters": {
        "options": {},
        "assignments": {
          "assignments": [
            {
              "id": "7e5f4c68-c155-4b7f-988a-678153b7dc88",
              "name": "text_C",
              "type": "string",
              "value": "={{ $json.text }}"
            }
          ]
        }
      },
      "typeVersion": 3.4
    },
    {
      "id": "72e0bf78-e133-41d7-8825-8edf9c9d7d6b",
      "name": "response d",
      "type": "n8n-nodes-base.set",
      "position": [
        -896,
        576
      ],
      "parameters": {
        "options": {},
        "assignments": {
          "assignments": [
            {
              "id": "c2f263f1-94d0-4e9f-857d-e103858d7a3b",
              "name": "text_D",
              "type": "string",
              "value": "={{ $json.text }}"
            }
          ]
        }
      },
      "typeVersion": 3.4
    },
    {
      "id": "5b505dd2-decf-4f3e-be31-83017fcb0e78",
      "name": "Code in JavaScript",
      "type": "n8n-nodes-base.code",
      "position": [
        816,
        64
      ],
      "parameters": {
        "jsCode": "const itemData = items[0].json; // ← ПРАВИЛЬНО!\n\nfunction parseRanking(text) {\n  const ranking = {};\n  const lines = text.split('\\n');\n  let inFinalRanking = false;\n  \n  lines.forEach(line => {\n    if (line.includes('FINAL RANKING')) {\n      inFinalRanking = true;\n    }\n    if (inFinalRanking && line.trim().length > 0) {\n      const match = line.match(/(\\d+)\\.\\s*(Response [A-D])/);\n      if (match) {\n        ranking[match[2]] = parseInt(match[1]);\n      }\n    }\n  });\n  \n  return ranking;\n}\n\nconst allRankings = [];\n['Raiting_1', 'Raiting_2', 'Raiting_3', 'Raiting_4'].forEach(key => {\n  if (itemData[key]) {\n    const parsed = parseRanking(itemData[key]);\n    allRankings.push(parsed);\n    console.log(`Parsed ${key}:`, parsed);\n  }\n});\n\nconst aggregated = {};\n['Response A', 'Response B', 'Response C', 'Response D'].forEach(resp => {\n  aggregated[resp] = { positions: [] };\n});\n\nallRankings.forEach(ranking => {\n  Object.entries(ranking).forEach(([response, position]) => {\n    if (aggregated[response]) {\n      aggregated[response].positions.push(position);\n    }\n  });\n});\n\nObject.keys(aggregated).forEach(response => {\n  const positions = aggregated[response].positions;\n  if (positions.length > 0) {\n    const avg = positions.reduce((a,b) => a+b, 0) / positions.length;\n    aggregated[response].average = parseFloat(avg.toFixed(2));\n    aggregated[response].count = positions.length;\n    aggregated[response].total = positions.reduce((a,b) => a+b, 0);\n  }\n});\n\nconst sorted = Object.entries(aggregated)\n  .sort((a, b) => a[1].average - b[1].average)\n  .reduce((obj, [key, val]) => { obj[key] = val; return obj; }, {});\n\nreturn [{\n  json: {\n    raw_rankings: allRankings,\n    aggregated_rankings: sorted,\n    best_response: Object.keys(sorted)[0],\n    best_score: sorted[Object.keys(sorted)[0]].average,\n    rankings_count: allRankings.length\n  }\n}];\n"
      },
      "typeVersion": 2
    },
    {
      "id": "9bb3a92e-55cc-480d-b32f-7c2d098071bd",
      "name": "claude3",
      "type": "@n8n/n8n-nodes-langchain.lmChatOpenRouter",
      "position": [
        -1216,
        -288
      ],
      "parameters": {
        "model": "anthropic/claude-sonnet-4.5",
        "options": {
          "maxTokens": 4000,
          "temperature": 0.7
        }
      },
      "credentials": {
        "openRouterApi": {
          "id": "1c8GWQkBiu8PxBdr",
          "name": "OpenRouter account"
        }
      },
      "typeVersion": 1
    },
    {
      "id": "100c79d2-8d3f-40f2-8a79-5cbc347eac81",
      "name": "openAI2",
      "type": "@n8n/n8n-nodes-langchain.lmChatOpenRouter",
      "position": [
        -1216,
        80
      ],
      "parameters": {
        "model": "openai/gpt-5.1",
        "options": {
          "maxTokens": 4000,
          "temperature": 0.7
        }
      },
      "credentials": {
        "openRouterApi": {
          "id": "1c8GWQkBiu8PxBdr",
          "name": "OpenRouter account"
        }
      },
      "typeVersion": 1
    },
    {
      "id": "f695dfab-5eb3-4d70-8112-7a3ccd2cb6b5",
      "name": "groq1",
      "type": "@n8n/n8n-nodes-langchain.lmChatOpenRouter",
      "position": [
        -1216,
        400
      ],
      "parameters": {
        "model": "x-ai/grok-4",
        "options": {
          "maxTokens": 4000,
          "temperature": 0.7
        }
      },
      "credentials": {
        "openRouterApi": {
          "id": "1c8GWQkBiu8PxBdr",
          "name": "OpenRouter account"
        }
      },
      "typeVersion": 1
    },
    {
      "id": "13769759-f6e6-4030-bd40-f447239d4d71",
      "name": "Merge5",
      "type": "n8n-nodes-base.merge",
      "position": [
        -208,
        -112
      ],
      "parameters": {
        "mode": "combine",
        "options": {},
        "combineBy": "combineByPosition",
        "numberInputs": 4
      },
      "typeVersion": 3.2
    },
    {
      "id": "894a1d5e-b727-4ddb-8631-6988d9f403df",
      "name": "claude4",
      "type": "@n8n/n8n-nodes-langchain.lmChatOpenRouter",
      "position": [
        1088,
        224
      ],
      "parameters": {
        "model": "google/gemini-3-pro-preview",
        "options": {
          "temperature": 0.6
        }
      },
      "credentials": {
        "openRouterApi": {
          "id": "1c8GWQkBiu8PxBdr",
          "name": "OpenRouter account"
        }
      },
      "typeVersion": 1
    },
    {
      "id": "e2bd5116-4489-46ee-9108-b641446301d8",
      "name": "evaluate claude",
      "type": "@n8n/n8n-nodes-langchain.chainLlm",
      "position": [
        -16,
        -400
      ],
      "parameters": {
        "text": "=You are evaluating different responses to the following question:\n\nQuestion: {{ $('When chat message received').item.json.chatInput }}\n\nHere are the responses from different models (anonymized):\n\nResponse A:\n{{ $json.text_A }}\n\nResponse B:\n{{ $json.text_B }}\n\nResponse C:\n{{ $json.text_C }}\n\nResponse D:\n{{ $json.text_D }}\n\nYour task:\n1. First, evaluate each response individually. For each response, explain what it does well and what it does poorly.\n2. Then, at the very end of your response, provide a final ranking.\n\nIMPORTANT: Your final ranking MUST be formatted EXACTLY as follows:\n- Start with the line \"FINAL RANKING:\" (all caps, with colon)\n- Then list the responses from best to worst as a numbered list\n- Each line should be: number, period, space, then ONLY the response label (e.g., \"1. Response A\")\n- Do not add any other text or explanations in the ranking section\n- If there is an error with the data, please report it immediately and do not generate any response.\n- give answer on english\n\n\nExample of the correct format for your ENTIRE response:\n\nResponse A provides good detail on X but misses Y...\nResponse B is accurate but lacks depth on Z...\nResponse C offers the most comprehensive answer...\n\nFINAL RANKING:\n1. Response C\n2. Response A\n3. Response B\n4. Response D\n\nNow provide your evaluation and ranking:\n\n",
        "batching": {},
        "promptType": "define"
      },
      "typeVersion": 1.7
    },
    {
      "id": "de6f093f-e113-4028-8ff5-d99ed5cbf97a",
      "name": "evaluate grok",
      "type": "@n8n/n8n-nodes-langchain.chainLlm",
      "position": [
        -16,
        224
      ],
      "parameters": {
        "text": "=You are evaluating different responses to the following question:\n\nQuestion: {{ $('When chat message received').item.json.chatInput }}\n\nHere are the responses from different models (anonymized):\n\nResponse A:\n{{ $json.text_A }}\n\nResponse B:\n{{ $json.text_B }}\n\nResponse C:\n{{ $json.text_C }}\n\nResponse D:\n{{ $json.text_D }}\n\nYour task:\n1. First, evaluate each response individually. For each response, explain what it does well and what it does poorly.\n2. Then, at the very end of your response, provide a final ranking.\n\nIMPORTANT: Your final ranking MUST be formatted EXACTLY as follows:\n- Start with the line \"FINAL RANKING:\" (all caps, with colon)\n- Then list the responses from best to worst as a numbered list\n- Each line should be: number, period, space, then ONLY the response label (e.g., \"1. Response A\")\n- Do not add any other text or explanations in the ranking section\n- If there is an error with the data, please report it immediately and do not generate any response.\n- give answer on english\n\n\nExample of the correct format for your ENTIRE response:\n\nResponse A provides good detail on X but misses Y...\nResponse B is accurate but lacks depth on Z...\nResponse C offers the most comprehensive answer...\n\nFINAL RANKING:\n1. Response C\n2. Response A\n3. Response B\n4. Response D\n\nNow provide your evaluation and ranking:\n\n",
        "batching": {},
        "promptType": "define"
      },
      "typeVersion": 1.7
    },
    {
      "id": "26468bc2-7162-497c-8563-847e8e84d5db",
      "name": "evaluate gemini",
      "type": "@n8n/n8n-nodes-langchain.chainLlm",
      "position": [
        -16,
        544
      ],
      "parameters": {
        "text": "=You are evaluating different responses to the following question:\n\nQuestion: {{ $('When chat message received').item.json.chatInput }}\n\nHere are the responses from different models (anonymized):\n\nResponse A:\n{{ $json.text_A }}\n\nResponse B:\n{{ $json.text_B }}\n\nResponse C:\n{{ $json.text_C }}\n\nResponse D:\n{{ $json.text_D }}\n\nYour task:\n1. First, evaluate each response individually. For each response, explain what it does well and what it does poorly.\n2. Then, at the very end of your response, provide a final ranking.\n\nIMPORTANT: Your final ranking MUST be formatted EXACTLY as follows:\n- Start with the line \"FINAL RANKING:\" (all caps, with colon)\n- Then list the responses from best to worst as a numbered list\n- Each line should be: number, period, space, then ONLY the response label (e.g., \"1. Response A\")\n- Do not add any other text or explanations in the ranking section\n- If there is an error with the data, please report it immediately and do not generate any response.\n- give answer on english\n\n\nExample of the correct format for your ENTIRE response:\n\nResponse A provides good detail on X but misses Y...\nResponse B is accurate but lacks depth on Z...\nResponse C offers the most comprehensive answer...\n\nFINAL RANKING:\n1. Response C\n2. Response A\n3. Response B\n4. Response D\n\nNow provide your evaluation and ranking:\n\n",
        "batching": {},
        "promptType": "define"
      },
      "typeVersion": 1.7
    },
    {
      "id": "365bc7bc-b8d8-47c1-8169-6eddecbebbdd",
      "name": "evaluate gpt",
      "type": "@n8n/n8n-nodes-langchain.chainLlm",
      "position": [
        -16,
        -80
      ],
      "parameters": {
        "text": "=You are evaluating different responses to the following question:\n\nQuestion: {{ $('When chat message received').item.json.chatInput }}\n\nHere are the responses from different models (anonymized):\n\nResponse A:\n{{ $json.text_A }}\n\nResponse B:\n{{ $json.text_B }}\n\nResponse C:\n{{ $json.text_C }}\n\nResponse D:\n{{ $json.text_D }}\n\nYour task:\n1. First, evaluate each response individually. For each response, explain what it does well and what it does poorly.\n2. Then, at the very end of your response, provide a final ranking.\n\nIMPORTANT: Your final ranking MUST be formatted EXACTLY as follows:\n- Start with the line \"FINAL RANKING:\" (all caps, with colon)\n- Then list the responses from best to worst as a numbered list\n- Each line should be: number, period, space, then ONLY the response label (e.g., \"1. Response A\")\n- Do not add any other text or explanations in the ranking section\n- If there is an error with the data, please report it immediately and do not generate any response.\n- give answer on english\n\n\nExample of the correct format for your ENTIRE response:\n\nResponse A provides good detail on X but misses Y...\nResponse B is accurate but lacks depth on Z...\nResponse C offers the most comprehensive answer...\n\nFINAL RANKING:\n1. Response C\n2. Response A\n3. Response B\n4. Response D\n\nNow provide your evaluation and ranking:\n\n",
        "batching": {},
        "promptType": "define"
      },
      "typeVersion": 1.7
    },
    {
      "id": "ae986c1c-e2c5-456e-98c7-b6d5018acb2e",
      "name": "evaluate_a",
      "type": "n8n-nodes-base.set",
      "position": [
        304,
        -400
      ],
      "parameters": {
        "options": {},
        "assignments": {
          "assignments": [
            {
              "id": "6c544736-80c2-4f52-b4b9-5236f28f524c",
              "name": "Raiting_1",
              "type": "string",
              "value": "={{ $json.text }}"
            }
          ]
        }
      },
      "typeVersion": 3.4
    },
    {
      "id": "c03b9556-9260-4776-8498-4d971a447c28",
      "name": "evaluate_b",
      "type": "n8n-nodes-base.set",
      "position": [
        304,
        -80
      ],
      "parameters": {
        "options": {},
        "assignments": {
          "assignments": [
            {
              "id": "5df1b7b3-57e0-43a0-9214-70739d8cea8a",
              "name": "Raiting_2",
              "type": "string",
              "value": "={{ $json.text }}"
            }
          ]
        }
      },
      "typeVersion": 3.4
    },
    {
      "id": "449c03be-775b-42cd-b329-d021666a10c6",
      "name": "evaluate_c",
      "type": "n8n-nodes-base.set",
      "position": [
        304,
        224
      ],
      "parameters": {
        "options": {},
        "assignments": {
          "assignments": [
            {
              "id": "21e8321a-1317-4e5b-bfdb-38947be54ee2",
              "name": "Raiting_3",
              "type": "string",
              "value": "={{ $json.text }}"
            }
          ]
        }
      },
      "typeVersion": 3.4
    },
    {
      "id": "81291e94-9d74-4be7-b801-d52dadb97a33",
      "name": "evaluate_d",
      "type": "n8n-nodes-base.set",
      "position": [
        304,
        544
      ],
      "parameters": {
        "options": {},
        "assignments": {
          "assignments": [
            {
              "id": "d1f431e9-7a2a-4956-b134-ec439d956109",
              "name": "Raiting_4",
              "type": "string",
              "value": "={{ $json.text }}"
            }
          ]
        }
      },
      "typeVersion": 3.4
    },
    {
      "id": "e10bd59b-df65-4f06-b01c-12e5c9718ee5",
      "name": "claude",
      "type": "@n8n/n8n-nodes-langchain.lmChatOpenRouter",
      "position": [
        -16,
        -240
      ],
      "parameters": {
        "model": "anthropic/claude-sonnet-4.5",
        "options": {
          "maxTokens": 2000,
          "temperature": 0.3
        }
      },
      "credentials": {
        "openRouterApi": {
          "id": "1c8GWQkBiu8PxBdr",
          "name": "OpenRouter account"
        }
      },
      "typeVersion": 1
    },
    {
      "id": "e327a8cb-6eb9-4387-869f-1ebfd2bd8035",
      "name": "openAI",
      "type": "@n8n/n8n-nodes-langchain.lmChatOpenRouter",
      "position": [
        -16,
        96
      ],
      "parameters": {
        "model": "openai/gpt-5.1",
        "options": {
          "maxTokens": 2000,
          "temperature": 0.3
        }
      },
      "credentials": {
        "openRouterApi": {
          "id": "1c8GWQkBiu8PxBdr",
          "name": "OpenRouter account"
        }
      },
      "typeVersion": 1
    },
    {
      "id": "716101d0-c2a8-4266-8931-1d7805cd1152",
      "name": "groq",
      "type": "@n8n/n8n-nodes-langchain.lmChatOpenRouter",
      "position": [
        -16,
        400
      ],
      "parameters": {
        "model": "x-ai/grok-4",
        "options": {
          "maxTokens": 2000,
          "temperature": 0.3
        }
      },
      "credentials": {
        "openRouterApi": {
          "id": "1c8GWQkBiu8PxBdr",
          "name": "OpenRouter account"
        }
      },
      "typeVersion": 1
    },
    {
      "id": "375d1fe2-f040-498f-806d-dbf40a123e99",
      "name": "gemini2",
      "type": "@n8n/n8n-nodes-langchain.lmChatOpenRouter",
      "position": [
        -16,
        704
      ],
      "parameters": {
        "model": "google/gemini-3-flash-preview",
        "options": {
          "maxTokens": 2000,
          "temperature": 0.3
        }
      },
      "credentials": {
        "openRouterApi": {
          "id": "1c8GWQkBiu8PxBdr",
          "name": "OpenRouter account"
        }
      },
      "typeVersion": 1
    },
    {
      "id": "80fa8e7d-4e00-48f7-b608-7cc55e55b5b0",
      "name": "Sticky Note2",
      "type": "n8n-nodes-base.stickyNote",
      "position": [
        -1232,
        -656
      ],
      "parameters": {
        "color": 6,
        "width": 256,
        "height": 1552,
        "content": "## Stage 1\nGet answers from different models"
      },
      "typeVersion": 1
    },
    {
      "id": "7b289f3b-689e-40b7-b227-8dd61a59a27d",
      "name": "Sticky Note3",
      "type": "n8n-nodes-base.stickyNote",
      "position": [
        -960,
        -656
      ],
      "parameters": {
        "color": 6,
        "height": 1552,
        "content": "## Stage 2\nFor convenience and impartiality, we store and anonymize responses."
      },
      "typeVersion": 1
    },
    {
      "id": "76699f65-6b7c-43d7-a4c6-bf73dc3f18d2",
      "name": "Sticky Note4",
      "type": "n8n-nodes-base.stickyNote",
      "position": [
        -288,
        -656
      ],
      "parameters": {
        "color": 6,
        "height": 1552,
        "content": "## For convenience and impartiality, we store and anonymize responses."
      },
      "typeVersion": 1
    },
    {
      "id": "3e03a8aa-ce45-4db2-906d-a83c28e6b05e",
      "name": "Sticky Note5",
      "type": "n8n-nodes-base.stickyNote",
      "position": [
        -32,
        -656
      ],
      "parameters": {
        "color": 6,
        "width": 256,
        "height": 1552,
        "content": "## Stage 3\nFor convenience and impartiality, we store and anonymize responses."
      },
      "typeVersion": 1
    },
    {
      "id": "509b338b-6145-465f-ae89-691dd4268cac",
      "name": "Sticky Note6",
      "type": "n8n-nodes-base.stickyNote",
      "position": [
        240,
        -656
      ],
      "parameters": {
        "color": 6,
        "height": 1552,
        "content": "## we get an analysis of all the answers"
      },
      "typeVersion": 1
    },
    {
      "id": "85243108-8cd7-4ea6-8052-4bee850b19b4",
      "name": "Sticky Note7",
      "type": "n8n-nodes-base.stickyNote",
      "position": [
        752,
        -96
      ],
      "parameters": {
        "color": 6,
        "height": 432,
        "content": "## Stage 4\nThis is where the magic happens and all the scores are ranked."
      },
      "typeVersion": 1
    },
    {
      "id": "32e6d9fe-1f5b-44f5-8e9b-392b5dcd24e3",
      "name": "Sticky Note8",
      "type": "n8n-nodes-base.stickyNote",
      "position": [
        1040,
        -96
      ],
      "parameters": {
        "color": 6,
        "width": 304,
        "height": 432,
        "content": "## Stage 5\nAll responses are evaluated by one model and a generalized and summarized response is provided."
      },
      "typeVersion": 1
    },
    {
      "id": "26f6efd1-99be-4851-bc4c-ee907062e511",
      "name": "Anthropic Chat Model",
      "type": "@n8n/n8n-nodes-langchain.lmChatAnthropic",
      "position": [
        -1680,
        704
      ],
      "parameters": {
        "model": {
          "__rl": true,
          "mode": "list",
          "value": "claude-sonnet-4-5-20250929",
          "cachedResultName": "Claude Sonnet 4.5"
        },
        "options": {}
      },
      "credentials": {
        "anthropicApi": {
          "id": "5yctPGa2dEHOEhly",
          "name": "Test Claud"
        }
      },
      "typeVersion": 1.3
    },
    {
      "id": "e7ff77c9-6ae6-45af-b761-973dffb7612f",
      "name": "xAI Grok Chat Model",
      "type": "@n8n/n8n-nodes-langchain.lmChatXAiGrok",
      "position": [
        -1520,
        704
      ],
      "parameters": {
        "options": {}
      },
      "credentials": {
        "xAiApi": {
          "id": "cgRIQbYSYWLb8bYv",
          "name": "Test Grok"
        }
      },
      "typeVersion": 1
    },
    {
      "id": "d4e47a9a-122e-4e0d-933c-142054d0a0eb",
      "name": "OpenAI Chat Model",
      "type": "@n8n/n8n-nodes-langchain.lmChatOpenAi",
      "position": [
        -1520,
        544
      ],
      "parameters": {
        "model": {
          "__rl": true,
          "mode": "list",
          "value": "gpt-4.1-mini"
        },
        "options": {},
        "builtInTools": {}
      },
      "credentials": {
        "openAiApi": {
          "id": "mILSsq03xGUAjYqe",
          "name": "Test"
        }
      },
      "typeVersion": 1.3
    },
    {
      "id": "7f89855e-ba74-4fd7-b0a5-131baf63f03e",
      "name": "Google Gemini Chat Model",
      "type": "@n8n/n8n-nodes-langchain.lmChatGoogleGemini",
      "position": [
        -1680,
        544
      ],
      "parameters": {
        "options": {}
      },
      "credentials": {
        "googlePalmApi": {
          "id": "4fkTqSVMFBOr1i2z",
          "name": "Test"
        }
      },
      "typeVersion": 1
    },
    {
      "id": "2a7ee75f-d680-425e-9916-f66ec75be81c",
      "name": "Sticky Note9",
      "type": "n8n-nodes-base.stickyNote",
      "position": [
        -1712,
        432
      ],
      "parameters": {
        "color": 7,
        "width": 352,
        "height": 416,
        "content": "## possible options for data analysis directly"
      },
      "typeVersion": 1
    },
    {
      "id": "e36c9d6b-dd88-4b09-9a46-63fe2783db9f",
      "name": "Sticky Note",
      "type": "n8n-nodes-base.stickyNote",
      "position": [
        -2144,
        432
      ],
      "parameters": {
        "color": 7,
        "width": 416,
        "height": 416,
        "content": "## possible data entry options"
      },
      "typeVersion": 1
    },
    {
      "id": "1848ca5f-2c79-4d4e-865f-90e4363e9ba0",
      "name": "Send a message1",
      "type": "n8n-nodes-base.gmail",
      "position": [
        -1920,
        512
      ],
      "webhookId": "d7eedc07-e729-4df0-9041-1d3879f3e32a",
      "parameters": {
        "options": {}
      },
      "credentials": {
        "gmailOAuth2": {
          "id": "WThxCOY6xEu8FWYd",
          "name": "Test GMail"
        }
      },
      "typeVersion": 2.2
    },
    {
      "id": "2e723348-7986-4097-a164-f41ca166cd4d",
      "name": "Send a message",
      "type": "n8n-nodes-base.slack",
      "position": [
        -1920,
        688
      ],
      "webhookId": "ad11040a-de88-4c1a-8651-f9871187b212",
      "parameters": {
        "otherOptions": {}
      },
      "typeVersion": 2.4
    },
    {
      "id": "0c047a14-3ccf-456a-83ac-0f616cbcc5e4",
      "name": "Send message",
      "type": "n8n-nodes-base.whatsApp",
      "position": [
        -2112,
        688
      ],
      "webhookId": "7ca1a720-2fc8-476f-92ab-3bd0b71f59bc",
      "parameters": {
        "operation": "send",
        "additionalFields": {}
      },
      "typeVersion": 1.1
    },
    {
      "id": "e75a552e-82b5-4eba-b201-93586e88ff79",
      "name": "Send a text message",
      "type": "n8n-nodes-base.telegram",
      "position": [
        -2112,
        512
      ],
      "webhookId": "f6f16bd1-f2ad-4713-aaaa-459f86763201",
      "parameters": {
        "additionalFields": {}
      },
      "credentials": {
        "telegramApi": {
          "id": "HJQKfpYJoO6wuDRR",
          "name": "Test"
        }
      },
      "typeVersion": 1.2
    },
    {
      "id": "c86237dd-e0a6-4228-9dd0-1ab04bf1fdcd",
      "name": "Sticky Note10",
      "type": "n8n-nodes-base.stickyNote",
      "position": [
        -2960,
        -352
      ],
      "parameters": {
        "width": 784,
        "height": 576,
        "content": "## How It Works\n\n1️⃣ User Input\nA user sends a single question via chat. This message becomes the shared input for the entire workflow.\n\n2️⃣ Parallel LLM Responses\nThe same question is sent simultaneously to multiple large language models.\nEach model generates its own answer independently, using the same prompt but its own reasoning.\n\n3️⃣ Response Anonymization\nAll model outputs are stored as anonymous responses (Response A–D).\nThis prevents evaluator models from knowing which LLM produced which answer.\n\n4️⃣ Peer Evaluation & Ranking\nEach LLM reviews all anonymized responses, analyzes their strengths and weaknesses, and produces a strict ranking from best to worst.\n\n5️⃣ Ranking Aggregation\nAll rankings are collected and aggregated.\nThe workflow calculates average positions and identifies the strongest response based on collective evaluation.\n\n6️⃣ Final Consensus Answer\nA dedicated “Chairman” model reviews all responses and rankings, detects agreement and disagreement patterns, and synthesizes one clear, high-quality final answer.\n\n➡️ Result: a consensus-driven response built from multiple independent LLM perspectives, not a single-model output.\n\n"
      },
      "typeVersion": 1
    },
    {
      "id": "d129dc4f-0ec2-46b2-8b83-e4bec8426df2",
      "name": "Sticky Note1",
      "type": "n8n-nodes-base.stickyNote",
      "position": [
        -2960,
        256
      ],
      "parameters": {
        "color": 3,
        "width": 784,
        "height": 592,
        "content": "## Setup Steps\n1️⃣ Import the Workflow\nImport the provided n8n workflow template into your instance.\n\n2️⃣ Configure OpenRouter Credentials\nAdd your OpenRouter API key in n8n credentials and connect it to all LLM nodes in the workflow.\n\n3️⃣ Select LLM Models\nChoose which models will participate in the council (e.g. GPT, Claude, Gemini, Grok).\nYou can add or remove models without changing the core logic.\n\n4️⃣ Review Prompt Nodes\nCheck the system and evaluation prompts.\nAdjust tone, language, or strictness if your use case requires it.\n\n5️⃣ Verify the Aggregation Code Node\nEnsure the JavaScript aggregation node matches the number of active LLMs and expected ranking format.\n\n6️⃣ Test with a Sample Question\nRun the workflow with a simple input to confirm that:\n\nAll models respond\n\nRankings are generated\n\nA final consensus answer is produced\n\n7️⃣ Activate the Workflow\nOnce verified, activate the workflow and use it in production or connect it to other automations."
      },
      "typeVersion": 1
    }
  ],
  "active": false,
  "pinData": {},
  "settings": {
    "executionOrder": "v1"
  },
  "versionId": "d50be5bd-4366-46ca-a785-c924dde43b65",
  "connections": {
    "groq": {
      "ai_languageModel": [
        [
          {
            "node": "evaluate grok",
            "type": "ai_languageModel",
            "index": 0
          }
        ]
      ]
    },
    "Merge": {
      "main": [
        [
          {
            "node": "evaluate gemini",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "groq1": {
      "ai_languageModel": [
        [
          {
            "node": "Basic LLM Chain2",
            "type": "ai_languageModel",
            "index": 0
          }
        ]
      ]
    },
    "Merge1": {
      "main": [
        [
          {
            "node": "evaluate grok",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Merge3": {
      "main": [
        [
          {
            "node": "evaluate claude",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Merge4": {
      "main": [
        [
          {
            "node": "Code in JavaScript",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Merge5": {
      "main": [
        [
          {
            "node": "evaluate gpt",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "claude": {
      "ai_languageModel": [
        [
          {
            "node": "evaluate claude",
            "type": "ai_languageModel",
            "index": 0
          }
        ]
      ]
    },
    "openAI": {
      "ai_languageModel": [
        [
          {
            "node": "evaluate gpt",
            "type": "ai_languageModel",
            "index": 0
          }
        ]
      ]
    },
    "claude3": {
      "ai_languageModel": [
        [
          {
            "node": "Basic LLM Chain",
            "type": "ai_languageModel",
            "index": 0
          }
        ]
      ]
    },
    "claude4": {
      "ai_languageModel": [
        [
          {
            "node": "Basic LLM Chain8",
            "type": "ai_languageModel",
            "index": 0
          }
        ]
      ]
    },
    "gemini1": {
      "ai_languageModel": [
        [
          {
            "node": "Basic LLM Chain3",
            "type": "ai_languageModel",
            "index": 0
          }
        ]
      ]
    },
    "gemini2": {
      "ai_languageModel": [
        [
          {
            "node": "evaluate gemini",
            "type": "ai_languageModel",
            "index": 0
          }
        ]
      ]
    },
    "openAI2": {
      "ai_languageModel": [
        [
          {
            "node": "Basic LLM Chain1",
            "type": "ai_languageModel",
            "index": 0
          }
        ]
      ]
    },
    "evaluate_a": {
      "main": [
        [
          {
            "node": "Merge4",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "evaluate_b": {
      "main": [
        [
          {
            "node": "Merge4",
            "type": "main",
            "index": 1
          }
        ]
      ]
    },
    "evaluate_c": {
      "main": [
        [
          {
            "node": "Merge4",
            "type": "main",
            "index": 2
          }
        ]
      ]
    },
    "evaluate_d": {
      "main": [
        [
          {
            "node": "Merge4",
            "type": "main",
            "index": 3
          }
        ]
      ]
    },
    "response a": {
      "main": [
        [
          {
            "node": "Merge3",
            "type": "main",
            "index": 0
          },
          {
            "node": "Merge1",
            "type": "main",
            "index": 0
          },
          {
            "node": "Merge",
            "type": "main",
            "index": 0
          },
          {
            "node": "Merge5",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "response b": {
      "main": [
        [
          {
            "node": "Merge3",
            "type": "main",
            "index": 1
          },
          {
            "node": "Merge5",
            "type": "main",
            "index": 1
          },
          {
            "node": "Merge1",
            "type": "main",
            "index": 1
          },
          {
            "node": "Merge",
            "type": "main",
            "index": 1
          }
        ]
      ]
    },
    "response c": {
      "main": [
        [
          {
            "node": "Merge3",
            "type": "main",
            "index": 2
          },
          {
            "node": "Merge5",
            "type": "main",
            "index": 2
          },
          {
            "node": "Merge1",
            "type": "main",
            "index": 2
          },
          {
            "node": "Merge",
            "type": "main",
            "index": 2
          }
        ]
      ]
    },
    "response d": {
      "main": [
        [
          {
            "node": "Merge3",
            "type": "main",
            "index": 3
          },
          {
            "node": "Merge",
            "type": "main",
            "index": 3
          },
          {
            "node": "Merge1",
            "type": "main",
            "index": 3
          },
          {
            "node": "Merge5",
            "type": "main",
            "index": 3
          }
        ]
      ]
    },
    "evaluate gpt": {
      "main": [
        [
          {
            "node": "evaluate_b",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "evaluate grok": {
      "main": [
        [
          {
            "node": "evaluate_c",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Basic LLM Chain": {
      "main": [
        [
          {
            "node": "response a",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "evaluate claude": {
      "main": [
        [
          {
            "node": "evaluate_a",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "evaluate gemini": {
      "main": [
        [
          {
            "node": "evaluate_d",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Basic LLM Chain1": {
      "main": [
        [
          {
            "node": "response b",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Basic LLM Chain2": {
      "main": [
        [
          {
            "node": "response c",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Basic LLM Chain3": {
      "main": [
        [
          {
            "node": "response d",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Code in JavaScript": {
      "main": [
        [
          {
            "node": "Basic LLM Chain8",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "When chat message received": {
      "main": [
        [
          {
            "node": "Basic LLM Chain",
            "type": "main",
            "index": 0
          },
          {
            "node": "Basic LLM Chain1",
            "type": "main",
            "index": 0
          },
          {
            "node": "Basic LLM Chain2",
            "type": "main",
            "index": 0
          },
          {
            "node": "Basic LLM Chain3",
            "type": "main",
            "index": 0
          }
        ]
      ]
    }
  }
}