{
  "id": "kmQtg1DenmcMryFvi9N5D",
  "meta": {
    "site": "https://github.com/zengfr/n8n-workflow-all-templates",
    "name": "Multi-AI Council Research üîç: GPT 5.2, Claude Opus 4.6 & Gemini 3 Pro Aggregation",
    "wechat": "youandme10086",
    "id": 13261,
    "update_time": "2026-02-13"
  },
  "name": "Multi-AI Council Research",
  "tags": [],
  "nodes": [
    {
      "id": "4dd91365-c791-4252-8cd6-1f4e9ebcabd4",
      "name": "When chat message received",
      "type": "@n8n/n8n-nodes-langchain.chatTrigger",
      "position": [
        -832,
        304
      ],
      "webhookId": "20be4137-36b2-4d1e-b2d6-ae7bc2cb5026",
      "parameters": {
        "options": {
          "responseMode": "responseNodes"
        }
      },
      "typeVersion": 1.4
    },
    {
      "id": "d0ff1d3a-3108-4253-abd8-7b3b732d7a1d",
      "name": "Chat",
      "type": "@n8n/n8n-nodes-langchain.chat",
      "position": [
        432,
        1040
      ],
      "webhookId": "b3be76ab-1f12-4a06-81d6-b983f4a62408",
      "parameters": {
        "message": "This is not a research text. Please enter one.",
        "options": {
          "memoryConnection": false
        }
      },
      "typeVersion": 1.1
    },
    {
      "id": "5af81c5e-1f52-4935-8573-0ce887d4ff9e",
      "name": "Google Gemini Chat Model",
      "type": "@n8n/n8n-nodes-langchain.lmChatGoogleGemini",
      "position": [
        -432,
        496
      ],
      "parameters": {
        "options": {}
      },
      "credentials": {
        "googlePalmApi": {
          "id": "AaNPKXAphyMzRgfA",
          "name": "Google Gemini(PaLM) (Eure)"
        }
      },
      "typeVersion": 1
    },
    {
      "id": "8b722808-c2c0-43b0-96ae-1ebb42ca18b6",
      "name": "Structured Output Parser",
      "type": "@n8n/n8n-nodes-langchain.outputParserStructured",
      "position": [
        -224,
        496
      ],
      "parameters": {
        "schemaType": "manual",
        "inputSchema": "{\n\t\"type\": \"object\",\n\t\"properties\": {\n\t\t\"search\": {\n\t\t\t\"type\": \"string\"\n\t\t},\n\t\t\"query\": {\n\t\t\t\"type\": \"string\"\n\t\t}\n\t}\n}"
      },
      "typeVersion": 1.3
    },
    {
      "id": "5c5f2305-ebd8-480d-9beb-8ae2854ab56f",
      "name": "OpenAI Chat Model",
      "type": "@n8n/n8n-nodes-langchain.lmChatOpenAi",
      "position": [
        448,
        -208
      ],
      "parameters": {
        "model": {
          "__rl": true,
          "mode": "list",
          "value": "gpt-5.2-pro",
          "cachedResultName": "gpt-5.2-pro"
        },
        "options": {},
        "builtInTools": {
          "webSearch": {
            "searchContextSize": "medium"
          }
        }
      },
      "credentials": {
        "openAiApi": {
          "id": "TefveNaDaMERl1hY",
          "name": "OpenAi account (Eure)"
        }
      },
      "typeVersion": 1.3
    },
    {
      "id": "70ae5af6-884b-49d1-ada9-8a48069b04c8",
      "name": "ChatGPT 5.2",
      "type": "@n8n/n8n-nodes-langchain.chainLlm",
      "position": [
        416,
        -400
      ],
      "parameters": {
        "text": "={{ $json.output.query }}",
        "batching": {},
        "promptType": "define"
      },
      "typeVersion": 1.9
    },
    {
      "id": "09593191-9431-4658-84fd-4db0dd14aaa2",
      "name": "Claude Opus 4.6",
      "type": "@n8n/n8n-nodes-langchain.chainLlm",
      "position": [
        416,
        0
      ],
      "parameters": {
        "text": "={{ $json.output.query }}",
        "batching": {},
        "promptType": "define"
      },
      "typeVersion": 1.9
    },
    {
      "id": "733d016c-8c21-4300-a547-eda75e06cc46",
      "name": "Anthropic Chat Model",
      "type": "@n8n/n8n-nodes-langchain.lmChatAnthropic",
      "position": [
        448,
        176
      ],
      "parameters": {
        "model": {
          "__rl": true,
          "mode": "list",
          "value": "claude-opus-4-6",
          "cachedResultName": "Claude Opus 4.6"
        },
        "options": {
          "thinking": true,
          "thinkingBudget": 1024
        }
      },
      "credentials": {
        "anthropicApi": {
          "id": "NNTZAD0Gmf7lcniq",
          "name": "Anthropic account"
        }
      },
      "typeVersion": 1.3
    },
    {
      "id": "fb07c440-97ce-4c61-b874-ab4460e40878",
      "name": "Google Gemini Chat Model1",
      "type": "@n8n/n8n-nodes-langchain.lmChatGoogleGemini",
      "position": [
        432,
        560
      ],
      "parameters": {
        "options": {
          "maxOutputTokens": 2048
        },
        "modelName": "models/gemini-3-pro-preview"
      },
      "credentials": {
        "googlePalmApi": {
          "id": "AaNPKXAphyMzRgfA",
          "name": "Google Gemini(PaLM) (Eure)"
        }
      },
      "typeVersion": 1
    },
    {
      "id": "9afa4366-bc2f-4620-b9fe-ee07a21fdbdd",
      "name": "Gemini 3 Pro",
      "type": "@n8n/n8n-nodes-langchain.chainLlm",
      "position": [
        432,
        400
      ],
      "parameters": {
        "text": "={{ $json.output.query }}",
        "batching": {},
        "promptType": "define"
      },
      "typeVersion": 1.9
    },
    {
      "id": "2f4329a3-44ad-44e7-a359-37ea101cf7fc",
      "name": "ChatGPT Result",
      "type": "n8n-nodes-base.set",
      "position": [
        768,
        -400
      ],
      "parameters": {
        "options": {},
        "assignments": {
          "assignments": [
            {
              "id": "01d8253f-9878-428a-93d9-30b9bad4b6eb",
              "name": "chatgpt",
              "type": "string",
              "value": "={{ $json.text }}"
            }
          ]
        }
      },
      "typeVersion": 3.4
    },
    {
      "id": "4960b6de-9f37-40a6-a3f5-9f2c07e81ce9",
      "name": "Claude Result",
      "type": "n8n-nodes-base.set",
      "position": [
        768,
        0
      ],
      "parameters": {
        "options": {},
        "assignments": {
          "assignments": [
            {
              "id": "01d8253f-9878-428a-93d9-30b9bad4b6eb",
              "name": "claude",
              "type": "string",
              "value": "={{ $json.text }"
            }
          ]
        }
      },
      "typeVersion": 3.4
    },
    {
      "id": "5768118b-1126-4fb0-b162-c4f82abaaffb",
      "name": "Gemini Result",
      "type": "n8n-nodes-base.set",
      "position": [
        768,
        400
      ],
      "parameters": {
        "options": {},
        "assignments": {
          "assignments": [
            {
              "id": "01d8253f-9878-428a-93d9-30b9bad4b6eb",
              "name": "gemini",
              "type": "string",
              "value": "={{ $json.text }}"
            }
          ]
        }
      },
      "typeVersion": 3.4
    },
    {
      "id": "a49e7108-99cb-4a69-8776-23ea713f60f9",
      "name": "Google Gemini Chat Model2",
      "type": "@n8n/n8n-nodes-langchain.lmChatGoogleGemini",
      "position": [
        1264,
        192
      ],
      "parameters": {
        "options": {}
      },
      "credentials": {
        "googlePalmApi": {
          "id": "0p34rXqIqy8WuoPg",
          "name": "Google Gemini(PaLM) Api account"
        }
      },
      "typeVersion": 1
    },
    {
      "id": "862e1bd6-181e-4b88-b20d-73d379edf982",
      "name": "Multi-Response Aggregator",
      "type": "@n8n/n8n-nodes-langchain.chainLlm",
      "position": [
        1296,
        0
      ],
      "parameters": {
        "text": "=Response 1: {{ $json.chatgpt }}\n\nResponse 2: {{ $json.claude }}\n\nResponse 3: {{ $json.gemini }}",
        "batching": {},
        "messages": {
          "messageValues": [
            {
              "message": "You are an assistant specialized in intelligently aggregating responses from multiple sources or analyses.\n\n## Your Task\n\nYou will receive 3 separate responses that analyze the same question from different perspectives or using different methodologies. Your goal is to synthesize these responses into a single comprehensive, coherent, and well-structured answer.\n\n## Aggregation Guidelines\n\n### 1. Response Analysis\n- Identify common points across the 3 responses\n- Detect complementary information that integrates well together\n- Note any contradictions or divergences that require clarification\n\n### 2. Aggregated Response Structure\n- **Brief introduction**: Overview of the topic addressed\n- **Main body**: Integrate insights from all three sources, organizing by theme rather than by source\n- **Handling disagreements**: When responses conflict, present different perspectives transparently\n- **Conclusion**: Synthesize the key takeaways\n\n### 3. Quality Principles\n- **Completeness**: Include all relevant information from the 3 responses\n- **Coherence**: Create a fluid narrative, not a list of separate summaries\n- **Clarity**: Avoid redundancy while preserving nuance\n- **Attribution when relevant**: If sources have different confidence levels or expertise areas, note this\n- **Balanced perspective**: Don't favor one response unless there's clear reason to do so\n\n### 4. What to Avoid\n- Simply concatenating the three responses\n- Losing important details in over-summarization\n- Introducing information not present in the original responses\n- Creating false consensus when perspectives genuinely differ\n\n## Output Format\n\nProvide a single, unified response that reads as if written by one expert who has considered multiple analytical approaches. The reader should not be able to tell it came from 3 separate sources unless you explicitly note diverging viewpoints.\n\n---\n\n**Input Format Expected:**\n```\nResponse 1: [content]\nResponse 2: [content]\nResponse 3: [content]\n```\n\n**Your aggregated response should be comprehensive yet concise, professional, and directly useful to the end user.**"
            }
          ]
        },
        "promptType": "define"
      },
      "typeVersion": 1.9
    },
    {
      "id": "f2c333c9-c0bc-4e4b-8037-f19b8975e2ad",
      "name": "Search Query Optimizer",
      "type": "@n8n/n8n-nodes-langchain.chainLlm",
      "position": [
        -400,
        304
      ],
      "parameters": {
        "batching": {},
        "messages": {
          "messageValues": [
            {
              "message": "You are a specialized assistant that determines whether user input constitutes a search query or general conversational text, and optimizes search queries for AI agent interpretation.\n\n## Your Task\n\nAnalyze the user's input and:\n1. **Classify** whether it's a search query or general text\n2. **Optimize** the query if it's a search request\n3. **Return** a structured JSON response\n\n## Classification Criteria\n\n### It IS a Search Query when:\n- User explicitly asks to find, search, or look up information\n- Request implies information retrieval (e.g., \"What are the best...\", \"Show me...\", \"I need information about...\")\n- User asks about current events, facts, statistics, or data\n- Request requires external knowledge or recent information\n- Contains keywords like: \"search for\", \"find\", \"look up\", \"research\", \"what is\", \"how to\", \"where can I\"\n\n### It is NOT a Search Query when:\n- General conversation or greetings\n- Requests for explanations based on general knowledge\n- Creative writing requests\n- Coding or problem-solving tasks that don't require external data\n- Personal opinions or discussions\n- Task execution (e.g., \"write an email\", \"create a script\")\n\n## Query Optimization Process\n\nWhen you identify a search query, optimize it by:\n- **Removing conversational filler** (\"Can you please...\", \"I was wondering if...\")\n- **Extracting core keywords** and intent\n- **Adding specificity** if context allows\n- **Structuring for clarity** (e.g., \"best practices for X\" instead of \"what are some good ways to do X\")\n- **Preserving key constraints** (dates, locations, specific requirements)\n- **Making it concise** while retaining all important information\n\n## Output Format\n\nAlways respond with a JSON object in this exact format:\n\n```json\n{\n  \"research\": \"yes\",\n  \"query\": \"your optimized search query here\",\n  \"original_intent\": \"brief description of what user wants to find\"\n}\n```"
            }
          ]
        },
        "hasOutputParser": true
      },
      "typeVersion": 1.9
    },
    {
      "id": "11ab9c88-f235-4c80-8bce-85ebe4d3abca",
      "name": "Search?",
      "type": "n8n-nodes-base.if",
      "position": [
        -48,
        304
      ],
      "parameters": {
        "options": {},
        "conditions": {
          "options": {
            "version": 3,
            "leftValue": "",
            "caseSensitive": true,
            "typeValidation": "strict"
          },
          "combinator": "and",
          "conditions": [
            {
              "id": "48940d10-fde9-4d87-97aa-32e61492194a",
              "operator": {
                "type": "string",
                "operation": "equals"
              },
              "leftValue": "={{ $json.output.search }}",
              "rightValue": "yes"
            }
          ]
        }
      },
      "typeVersion": 2.3
    },
    {
      "id": "0a6a312d-2572-4176-8dc7-0db12a8c34b7",
      "name": "Sticky Note",
      "type": "n8n-nodes-base.stickyNote",
      "position": [
        -928,
        -336
      ],
      "parameters": {
        "width": 1024,
        "height": 464,
        "content": "## Multi-AI Council Research: GPT 5.2, Claude Opus 4.6 & Gemini 3 Pro Aggregation\nThis workflow implements a **multi-model AI orchestration** with the BEST models at now (**ChatGPT 5.2, Claude Opus 4.6, Gemini 3 Pro**) and response aggregation system designed to handle user chat inputs intelligently and reliably.\n\n### **How it works**\n\nThe workflow processes incoming chat messages through a Search Query Optimizer that classifies inputs as research queries or general conversation and optimizes research queries for accuracy. Research queries are then executed in parallel across three AI models‚ÄîChatGPT 5.2, Claude Opus 4.6, and Gemini 3 Pro‚Äîto reduce single-model bias and improve answer quality. Each model‚Äôs response is collected independently and passed to a Multi-Response Aggregator, which synthesizes them into one coherent, balanced output while reconciling or explaining contradictions. Non-research inputs bypass the multi-model flow and trigger a fallback message, ensuring efficient resource usage and predictable behavior.\n\n### **Setup steps**\n\nConfigure valid API credentials for OpenAI, Anthropic, and Google Gemini to enable all model nodes and query optimization. Verify workflow connections, including chat input to the Search Query Optimizer, conditional routing based on classification, parallel execution to the three models, and aggregation into the response synthesizer. Customize system prompts for the optimizer, aggregator, and individual model nodes to define classification logic, synthesis rules, and output formatting. Activate the workflow and test with multiple input types to confirm correct classification, parallel execution, aggregation behavior, and fallback handling for non-research queries.\n"
      },
      "typeVersion": 1
    },
    {
      "id": "8fcb79e1-54af-46a2-960a-41e1f4be3a6f",
      "name": "Sticky Note1",
      "type": "n8n-nodes-base.stickyNote",
      "position": [
        -512,
        160
      ],
      "parameters": {
        "color": 7,
        "width": 608,
        "height": 496,
        "content": "## STEP 1 - Input Processing\nWhen a chat message is received, it's sent to a \"Search Query Optimizer\" that determines whether the input is a research query or general conversation. "
      },
      "typeVersion": 1
    },
    {
      "id": "0bf9532d-5a01-4868-b383-a6b6562ce231",
      "name": "Sticky Note2",
      "type": "n8n-nodes-base.stickyNote",
      "position": [
        336,
        -528
      ],
      "parameters": {
        "color": 7,
        "width": 704,
        "height": 1360,
        "content": "## STEP2 - Multi-Model Query Execution\nIf the input is classified as a research query, the workflow simultaneously sends the optimized query to three different AI models: ChatGPT 5.2 (OpenAI), Claude Opus 4.6 (Anthropic), Gemini 3 Pro (Google)"
      },
      "typeVersion": 1
    },
    {
      "id": "3d176ee1-69d7-4793-be08-de7491bbc554",
      "name": "Sticky Note3",
      "type": "n8n-nodes-base.stickyNote",
      "position": [
        336,
        864
      ],
      "parameters": {
        "color": 7,
        "width": 704,
        "height": 352,
        "content": "## STEP 3 - Fallback Handling\nIf the input is not a research query, the workflow bypasses the multi-model execution and sends a default message asking the user to enter a research text."
      },
      "typeVersion": 1
    },
    {
      "id": "159ef060-a881-44c0-8145-d267808a7368",
      "name": "Sticky Note4",
      "type": "n8n-nodes-base.stickyNote",
      "position": [
        1184,
        -112
      ],
      "parameters": {
        "color": 7,
        "width": 704,
        "height": 464,
        "content": "## STEP 4 - Response Aggregation\nEach model's response is collected separately, then all three responses are sent to a \"Multi-Response Aggregator\" which synthesizes them into a single comprehensive answer."
      },
      "typeVersion": 1
    },
    {
      "id": "743dc0b7-e2c7-49b4-8f29-32ff130ec7ab",
      "name": "Sticky Note8",
      "type": "n8n-nodes-base.stickyNote",
      "position": [
        -1696,
        -608
      ],
      "parameters": {
        "color": 7,
        "width": 736,
        "height": 736,
        "content": "## MY NEW YOUTUBE CHANNEL\nüëâ [Subscribe to my new **YouTube channel**](https://youtube.com/@n3witalia). Here I‚Äôll share videos and Shorts with practical tutorials and **FREE templates for n8n**.\n\n[![image](https://n3wstorage.b-cdn.net/n3witalia/youtube-n8n-cover.jpg)](https://youtube.com/@n3witalia)"
      },
      "typeVersion": 1
    }
  ],
  "active": false,
  "pinData": {},
  "settings": {
    "binaryMode": "separate",
    "availableInMCP": false,
    "executionOrder": "v1"
  },
  "versionId": "8ed24e81-3b3a-4962-a3a9-74aa5d5408b9",
  "connections": {
    "Search?": {
      "main": [
        [
          {
            "node": "ChatGPT 5.2",
            "type": "main",
            "index": 0
          },
          {
            "node": "Claude Opus 4.6",
            "type": "main",
            "index": 0
          },
          {
            "node": "Gemini 3 Pro",
            "type": "main",
            "index": 0
          }
        ],
        [
          {
            "node": "Chat",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "ChatGPT 5.2": {
      "main": [
        [
          {
            "node": "ChatGPT Result",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Gemini 3 Pro": {
      "main": [
        [
          {
            "node": "Gemini Result",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Claude Result": {
      "main": [
        [
          {
            "node": "Multi-Response Aggregator",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Gemini Result": {
      "main": [
        [
          {
            "node": "Multi-Response Aggregator",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "ChatGPT Result": {
      "main": [
        [
          {
            "node": "Multi-Response Aggregator",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Claude Opus 4.6": {
      "main": [
        [
          {
            "node": "Claude Result",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "OpenAI Chat Model": {
      "ai_languageModel": [
        [
          {
            "node": "ChatGPT 5.2",
            "type": "ai_languageModel",
            "index": 0
          }
        ]
      ]
    },
    "Anthropic Chat Model": {
      "ai_languageModel": [
        [
          {
            "node": "Claude Opus 4.6",
            "type": "ai_languageModel",
            "index": 0
          }
        ]
      ]
    },
    "Search Query Optimizer": {
      "main": [
        [
          {
            "node": "Search?",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Google Gemini Chat Model": {
      "ai_languageModel": [
        [
          {
            "node": "Search Query Optimizer",
            "type": "ai_languageModel",
            "index": 0
          }
        ]
      ]
    },
    "Structured Output Parser": {
      "ai_outputParser": [
        [
          {
            "node": "Search Query Optimizer",
            "type": "ai_outputParser",
            "index": 0
          }
        ]
      ]
    },
    "Google Gemini Chat Model1": {
      "ai_languageModel": [
        [
          {
            "node": "Gemini 3 Pro",
            "type": "ai_languageModel",
            "index": 0
          }
        ]
      ]
    },
    "Google Gemini Chat Model2": {
      "ai_languageModel": [
        [
          {
            "node": "Multi-Response Aggregator",
            "type": "ai_languageModel",
            "index": 0
          }
        ]
      ]
    },
    "When chat message received": {
      "main": [
        [
          {
            "node": "Search Query Optimizer",
            "type": "main",
            "index": 0
          }
        ]
      ]
    }
  }
}